# Assessment Domain Glossary
# F5 Framework v2.0 - Education/Assessment
# Comprehensive terminology for assessment and psychometrics

glossary:
  domain: education
  subdomain: assessment
  version: "1.0.0"
  
  categories:
    - id: psychometrics
      name: Psychometrics
      name_ja: 心理測定学
      description: Statistical methods for analyzing test items and scores
      
    - id: scoring
      name: Scoring & Grading
      name_ja: 採点と評価
      description: Methods and concepts for evaluating responses
      
    - id: test_design
      name: Test Design
      name_ja: テスト設計
      description: Concepts related to exam construction
      
    - id: proctoring
      name: Proctoring
      name_ja: 監視
      description: Exam monitoring and security concepts
      
    - id: standards
      name: Standards & Formats
      name_ja: 規格とフォーマット
      description: Industry standards and data formats
      
    - id: statistics
      name: Statistics
      name_ja: 統計
      description: Statistical concepts used in assessment
      
    - id: accessibility
      name: Accessibility
      name_ja: アクセシビリティ
      description: Concepts related to accommodations and accessibility

  terms:
    # ==========================================
    # Psychometrics Terms
    # ==========================================
    - term: Difficulty Index
      term_ja: 困難度指数
      abbreviation: p-value
      category: psychometrics
      definition: |
        The proportion of test takers who answered a question correctly.
        Calculated as: p = (number correct) / (total responses)
      formula: "p = C / N"
      interpretation:
        - range: "0.9 - 1.0"
          meaning: Very easy
          action: Consider revising or retiring
        - range: "0.7 - 0.9"
          meaning: Easy
          action: Acceptable for mastery items
        - range: "0.4 - 0.7"
          meaning: Moderate
          action: Optimal difficulty range
        - range: "0.2 - 0.4"
          meaning: Hard
          action: Acceptable for advanced items
        - range: "0.0 - 0.2"
          meaning: Very hard
          action: Review for clarity issues
      example: "A difficulty index of 0.65 means 65% of candidates answered correctly"
      related_terms:
        - Discrimination Index
        - Item Analysis
        
    - term: Discrimination Index
      term_ja: 識別力指数
      abbreviation: D
      category: psychometrics
      definition: |
        A measure of how well a question differentiates between high and low 
        performing candidates. Calculated using upper and lower 27% groups.
      formula: "D = (U - L) / n"
      variables:
        U: "Number correct in upper 27% group"
        L: "Number correct in lower 27% group"
        n: "Number in each group"
      interpretation:
        - range: "0.4 - 1.0"
          meaning: Excellent discrimination
          action: Keep item
        - range: "0.3 - 0.4"
          meaning: Good discrimination
          action: Keep item
        - range: "0.2 - 0.3"
          meaning: Acceptable discrimination
          action: Consider revision
        - range: "0.0 - 0.2"
          meaning: Poor discrimination
          action: Revise or discard
        - range: "< 0"
          meaning: Negative discrimination
          action: Investigate - possible keying error
      related_terms:
        - Point-Biserial Correlation
        - Item Analysis
        
    - term: Point-Biserial Correlation
      term_ja: 点双列相関係数
      abbreviation: r_pb
      category: psychometrics
      definition: |
        A correlation coefficient measuring the relationship between a 
        dichotomous variable (correct/incorrect) and a continuous variable 
        (total score). Indicates how well item performance relates to overall 
        test performance.
      formula: "r_pb = (M_p - M_q) / S_t × √(p × q)"
      variables:
        M_p: "Mean total score for those who got item correct"
        M_q: "Mean total score for those who got item incorrect"
        S_t: "Standard deviation of total scores"
        p: "Proportion correct"
        q: "Proportion incorrect (1 - p)"
      interpretation:
        - range: "> 0.3"
          meaning: Strong positive relationship
        - range: "0.2 - 0.3"
          meaning: Moderate relationship
        - range: "< 0.2"
          meaning: Weak relationship
        - range: "< 0"
          meaning: Negative - investigate keying error
      related_terms:
        - Discrimination Index
        - Item-Total Correlation
        
    - term: Item Response Theory
      term_ja: 項目反応理論
      abbreviation: IRT
      category: psychometrics
      definition: |
        A family of mathematical models describing the relationship between 
        a test taker's ability (latent trait) and their probability of success 
        on individual items. Provides item-level analysis independent of sample.
      models:
        - name: Rasch Model (1PL)
          parameters: ["b (difficulty)"]
          use_case: Simple dichotomous items
          
        - name: Two-Parameter Model (2PL)
          parameters: ["a (discrimination)", "b (difficulty)"]
          use_case: Items with varying discrimination
          
        - name: Three-Parameter Model (3PL)
          parameters: ["a (discrimination)", "b (difficulty)", "c (guessing)"]
          use_case: Multiple choice with guessing factor
          
        - name: Graded Response Model (GRM)
          parameters: ["a (discrimination)", "b_k (thresholds)"]
          use_case: Polytomous (multi-point) items
      advantages:
        - Sample-independent item parameters
        - Ability-independent item parameters
        - Standard errors for each ability estimate
        - Support for adaptive testing
      related_terms:
        - Difficulty Index
        - Item Information Function
        - Theta
        
    - term: Differential Item Functioning
      term_ja: 項目機能差異
      abbreviation: DIF
      category: psychometrics
      definition: |
        Occurs when examinees from different groups with the same ability 
        have different probabilities of answering an item correctly. May 
        indicate potential bias in test items.
      types:
        - name: Uniform DIF
          description: Consistent difference across ability levels
        - name: Non-uniform DIF
          description: Difference varies by ability level
      detection_methods:
        - Mantel-Haenszel
        - Logistic regression
        - IRT-based methods
        - SIBTEST
      classification:
        - level: A (Negligible)
          delta_mh: "< 1.0"
          action: No action needed
        - level: B (Slight to Moderate)
          delta_mh: "1.0 - 1.5"
          action: Review item
        - level: C (Large)
          delta_mh: "> 1.5"
          action: Remove or revise item
      related_terms:
        - Item Bias
        - Fairness
        
    - term: Item Analysis
      term_ja: 項目分析
      category: psychometrics
      definition: |
        The process of examining response patterns to assess item quality, 
        including difficulty, discrimination, and distractor effectiveness. 
        Used to improve test items and overall test quality.
      components:
        - Difficulty analysis
        - Discrimination analysis
        - Distractor analysis
        - Reliability estimation
        - DIF analysis
      timing:
        - Pre-test piloting
        - Post-administration analysis
        - Ongoing monitoring
      outputs:
        - Item statistics report
        - Flagged items list
        - Recommendations for revision
      related_terms:
        - Difficulty Index
        - Discrimination Index
        - Distractor Analysis

    - term: Classical Test Theory
      term_ja: 古典的テスト理論
      abbreviation: CTT
      category: psychometrics
      definition: |
        A psychometric framework based on the idea that observed score equals 
        true score plus error. Forms the foundation of traditional test analysis.
      formula: "X = T + E"
      variables:
        X: "Observed score"
        T: "True score"
        E: "Error"
      assumptions:
        - Mean error is zero
        - Error is uncorrelated with true score
        - Errors on different tests are uncorrelated
      limitations:
        - Sample-dependent statistics
        - Test-dependent ability estimates
        - Equal treatment of all items
      related_terms:
        - Item Response Theory
        - Reliability
        - Standard Error of Measurement

    - term: Item Information Function
      term_ja: 項目情報関数
      abbreviation: IIF
      category: psychometrics
      definition: |
        In IRT, describes how much information an item provides at each 
        ability level. Higher information means more precise measurement.
      formula: "I(θ) = a² × P(θ) × Q(θ)"
      characteristics:
        - Peaked at item difficulty level
        - Higher discrimination = more information
        - Guessing parameter reduces information
      uses:
        - Item selection for adaptive testing
        - Test construction
        - Precision estimation
      related_terms:
        - Test Information Function
        - Standard Error of Measurement
        - IRT

    - term: Test Information Function
      term_ja: テスト情報関数
      abbreviation: TIF
      category: psychometrics
      definition: |
        The sum of item information functions across all items in a test. 
        Shows where the test measures most precisely across ability levels.
      formula: "I(θ) = Σ I_i(θ)"
      interpretation:
        - Higher values indicate more precise measurement
        - Shape reveals target ability range
        - Inverse related to standard error
      related_terms:
        - Item Information Function
        - Standard Error of Measurement
        
    # ==========================================
    # Scoring Terms
    # ==========================================
    - term: Scaled Score
      term_ja: 尺度得点
      category: scoring
      definition: |
        A transformation of raw scores to a standardized scale, allowing 
        comparison across different test forms or administrations.
      purpose:
        - Enable score comparison across forms
        - Account for difficulty differences
        - Maintain consistent meaning over time
      common_scales:
        - name: T-Score
          mean: 50
          sd: 10
        - name: Z-Score
          mean: 0
          sd: 1
        - name: IQ Scale
          mean: 100
          sd: 15
        - name: SAT Scale
          range: "200-800"
      process:
        - Collect raw scores
        - Apply equating procedure
        - Transform to scale
      related_terms:
        - Raw Score
        - Equating
        - Standard Score

    - term: Percentile Rank
      term_ja: パーセンタイル順位
      category: scoring
      definition: |
        The percentage of scores in a distribution that fall at or below 
        a given score. Indicates relative standing in a norm group.
      formula: "PR = (N_below + 0.5 × N_at) / N_total × 100"
      interpretation:
        - 50th percentile = median
        - 75th percentile = top quarter
        - 99th percentile = top 1%
      cautions:
        - Not equal intervals
        - Cannot be averaged
        - Norm-group dependent
      related_terms:
        - Norm-Referenced Scoring
        - Standard Score

    - term: Raw Score
      term_ja: 素点
      category: scoring
      definition: |
        The unadjusted count of correct responses or points earned on a test 
        before any transformation or scaling is applied.
      calculation_methods:
        - Number correct
        - Sum of item scores
        - Formula scoring (with penalty)
      limitations:
        - Not comparable across test forms
        - Does not account for item difficulty
        - May include guessing effects
      related_terms:
        - Scaled Score
        - Standard Score

    - term: Cut Score
      term_ja: カットスコア
      category: scoring
      definition: |
        A specific point on a score scale that divides examinees into 
        categories (e.g., pass/fail, proficiency levels).
      setting_methods:
        - name: Angoff Method
          description: Expert judgment on borderline candidates
        - name: Bookmark Method
          description: Ordered item booklets with placement
        - name: Contrasting Groups
          description: Based on known group differences
        - name: Borderline Group
          description: Analysis of borderline performers
      considerations:
        - Validity evidence
        - Standard error
        - Consequences of misclassification
        - Legal defensibility
      related_terms:
        - Criterion-Referenced Scoring
        - Standard Setting
        
    - term: Rubric
      term_ja: ルーブリック
      category: scoring
      definition: |
        A scoring guide that describes criteria for evaluating performance 
        at different quality levels. Used for subjective assessment.
      types:
        - name: Analytic Rubric
          description: Separate scores for each criterion
          use_case: Detailed feedback
        - name: Holistic Rubric
          description: Single overall score
          use_case: Efficient grading
        - name: Single-Point Rubric
          description: Describes proficiency only
          use_case: Formative assessment
      components:
        - Criteria (dimensions being assessed)
        - Performance levels (quality categories)
        - Descriptors (specific characteristics)
        - Point values (scores for each level)
      best_practices:
        - Clear, observable criteria
        - Distinct level descriptions
        - Consistent language
        - Anchor samples for training
      related_terms:
        - Analytic Scoring
        - Holistic Scoring
        - Inter-Rater Reliability

    - term: Negative Marking
      term_ja: 減点法
      category: scoring
      definition: |
        A scoring method that penalizes incorrect answers to discourage 
        guessing. Points are subtracted for wrong responses.
      formula: "Score = C - (W / (k-1))"
      variables:
        C: "Number correct"
        W: "Number wrong"
        k: "Number of options per question"
      advantages:
        - Discourages random guessing
        - More accurate ability estimation
      disadvantages:
        - May disadvantage risk-averse test takers
        - Can increase test anxiety
      alternatives:
        - Number-right scoring
        - Omit option with no penalty
      related_terms:
        - Formula Scoring
        - Guessing Correction
        
    # ==========================================
    # Test Design Terms
    # ==========================================
    - term: Question and Test Interoperability
      term_ja: 問題・テスト相互運用性
      abbreviation: QTI
      category: standards
      definition: |
        An IMS Global standard for representing assessment content and results 
        in a vendor-neutral format. Enables interoperability between systems.
      versions:
        - version: "2.1"
          status: Widely adopted
        - version: "3.0"
          status: Current standard
      supported_item_types:
        - Choice (single/multiple)
        - Order
        - Associate
        - Extended text
        - Inline choice
        - Text entry
        - Slider
        - Drawing
      components:
        - Item metadata
        - Response declarations
        - Item body
        - Response processing
        - Outcome declarations
      related_terms:
        - LTI
        - Assessment Interoperability

    - term: Question Pool
      term_ja: 問題プール
      category: test_design
      definition: |
        A collection of questions from which items are randomly selected 
        to create exam instances. Enables test form variation.
      selection_strategies:
        - Pure random
        - Stratified random
        - Adaptive
        - Constrained random
      constraints:
        - Content coverage
        - Difficulty distribution
        - Enemy items (mutually exclusive)
        - Item exposure control
      advantages:
        - Increased test security
        - Reduced item exposure
        - Multiple equivalent forms
      related_terms:
        - Item Bank
        - Test Form
        - Parallel Forms

    - term: Test Blueprint
      term_ja: テスト設計図
      category: test_design
      definition: |
        A specification document that outlines the structure, content 
        coverage, and psychometric targets for a test.
      components:
        - Content domain mapping
        - Cognitive level distribution
        - Item type distribution
        - Difficulty targets
        - Time allocation
      purposes:
        - Ensure content validity
        - Guide item development
        - Maintain consistency across forms
      related_terms:
        - Content Validity
        - Test Specifications

    - term: Adaptive Testing
      term_ja: 適応型テスト
      abbreviation: CAT
      category: test_design
      definition: |
        Computer-based testing that adjusts item difficulty based on 
        examinee performance. Selects optimal items in real-time.
      components:
        - Item pool with IRT parameters
        - Item selection algorithm
        - Ability estimation method
        - Stopping rule
      advantages:
        - Shorter tests
        - More precise measurement
        - Reduced frustration
        - Enhanced security
      stopping_rules:
        - Fixed length
        - Standard error threshold
        - Maximum time
        - Content requirements met
      related_terms:
        - IRT
        - Item Information Function
        
    # ==========================================
    # Proctoring Terms
    # ==========================================
    - term: Proctoring
      term_ja: 監視
      category: proctoring
      definition: |
        The supervision of test takers during an examination to ensure 
        test integrity and prevent cheating. Can be in-person or remote.
      types:
        - name: Live Proctoring
          description: Real-time human monitoring
          use_case: High-stakes exams
        - name: Recorded Proctoring
          description: Video review after exam
          use_case: Moderate-stakes exams
        - name: AI Proctoring
          description: Automated monitoring with AI
          use_case: Large-scale testing
        - name: Hybrid
          description: AI with human escalation
          use_case: Balanced approach
      monitored_elements:
        - Face detection and recognition
        - Eye tracking
        - Audio analysis
        - Browser behavior
        - Environment scanning
      related_terms:
        - Identity Verification
        - Test Security
        - Remote Testing

    - term: Identity Verification
      term_ja: 本人確認
      category: proctoring
      definition: |
        The process of confirming that the person taking an exam is who 
        they claim to be. Essential for test validity and security.
      methods:
        - name: Photo ID Check
          description: Compare ID document to candidate
        - name: Facial Recognition
          description: Biometric matching
        - name: Knowledge-Based Authentication
          description: Security questions
        - name: Multi-Factor
          description: Combined methods
      standards:
        - Government-issued ID required
        - Photo match verification
        - Continuous identity monitoring
      related_terms:
        - Proctoring
        - Authentication
        
    - term: Browser Lockdown
      term_ja: ブラウザロックダウン
      category: proctoring
      definition: |
        Technology that restricts browser and system functionality during 
        an exam to prevent access to unauthorized resources.
      features:
        - Disable tab switching
        - Block external applications
        - Prevent copy/paste
        - Disable screen capture
        - Block printing
        - Restrict keyboard shortcuts
      implementations:
        - Secure browser applications
        - Kiosk mode
        - Browser extensions
      related_terms:
        - Secure Browser
        - Test Security
        
    # ==========================================
    # Statistics Terms
    # ==========================================
    - term: Reliability
      term_ja: 信頼性
      category: statistics
      definition: |
        The consistency or stability of test scores. A reliable test 
        produces consistent results across repeated administrations.
      types:
        - name: Test-Retest Reliability
          description: Consistency over time
        - name: Internal Consistency
          description: Consistency among items
        - name: Inter-Rater Reliability
          description: Consistency among scorers
        - name: Parallel Forms
          description: Consistency across test forms
      coefficients:
        - name: Cronbach's Alpha
          range: "0 - 1"
          acceptable: "> 0.7"
        - name: KR-20
          use: Dichotomous items
        - name: Cohen's Kappa
          use: Inter-rater agreement
      related_terms:
        - Validity
        - Standard Error of Measurement

    - term: Validity
      term_ja: 妥当性
      category: statistics
      definition: |
        The extent to which a test measures what it is intended to measure. 
        Evidence that test score interpretations are appropriate.
      types:
        - name: Content Validity
          description: Coverage of intended domain
          evidence: Expert review, blueprint alignment
        - name: Construct Validity
          description: Measures intended construct
          evidence: Factor analysis, convergent/divergent
        - name: Criterion Validity
          description: Relates to external criteria
          evidence: Correlation with outcomes
        - name: Face Validity
          description: Appears to measure intended construct
          evidence: Stakeholder perception
      validation_process:
        - Define construct
        - Gather evidence
        - Evaluate interpretations
        - Document conclusions
      related_terms:
        - Reliability
        - Standard Setting

    - term: Cronbach's Alpha
      term_ja: クロンバックのアルファ
      abbreviation: α
      category: statistics
      definition: |
        A measure of internal consistency reliability. Indicates how well 
        items in a test measure the same underlying construct.
      formula: "α = (k / (k-1)) × (1 - Σσ²_i / σ²_t)"
      variables:
        k: "Number of items"
        σ²_i: "Variance of item i"
        σ²_t: "Total test variance"
      interpretation:
        - range: "> 0.9"
          meaning: Excellent
        - range: "0.8 - 0.9"
          meaning: Good
        - range: "0.7 - 0.8"
          meaning: Acceptable
        - range: "0.6 - 0.7"
          meaning: Questionable
        - range: "< 0.6"
          meaning: Poor
      cautions:
        - Affected by test length
        - Assumes tau-equivalence
        - Not appropriate for multidimensional tests
      related_terms:
        - Reliability
        - KR-20

    - term: Standard Error of Measurement
      term_ja: 測定の標準誤差
      abbreviation: SEM
      category: statistics
      definition: |
        An estimate of the amount of error in a test score. Indicates the 
        precision of measurement for individual scores.
      formula: "SEM = SD × √(1 - r)"
      variables:
        SD: "Standard deviation of test scores"
        r: "Reliability coefficient"
      uses:
        - Confidence intervals for scores
        - Score band interpretation
        - Pass/fail decision accuracy
      interpretation: |
        A true score likely falls within ±1 SEM of observed score 
        approximately 68% of the time
      related_terms:
        - Reliability
        - Confidence Interval
        
    - term: Theta
      term_ja: シータ
      abbreviation: θ
      category: statistics
      definition: |
        In IRT, the latent trait or ability being measured. Typically 
        scaled with mean of 0 and standard deviation of 1.
      scale:
        typical_range: "-3 to +3"
        mean: 0
        sd: 1
      interpretation:
        - θ = 0: Average ability
        - θ = 1: One SD above average
        - θ = -1: One SD below average
      estimation_methods:
        - Maximum Likelihood (MLE)
        - Expected A Posteriori (EAP)
        - Maximum A Posteriori (MAP)
      related_terms:
        - IRT
        - Ability Estimate
        
    # ==========================================
    # Accessibility Terms
    # ==========================================
    - term: Accommodation
      term_ja: 配慮措置
      category: accessibility
      definition: |
        Changes to testing conditions or format that enable test takers 
        with disabilities to demonstrate their knowledge without altering 
        what the test measures.
      types:
        - name: Timing
          examples: ["Extended time", "Extra breaks"]
        - name: Setting
          examples: ["Separate room", "Reduced distractions"]
        - name: Presentation
          examples: ["Large print", "Screen reader", "Braille"]
        - name: Response
          examples: ["Scribe", "Speech-to-text", "Alternate keyboard"]
      principles:
        - Must not change construct measured
        - Based on documented need
        - Applied consistently
        - Validated for use
      related_terms:
        - ADA Compliance
        - WCAG
        - Universal Design

    - term: Universal Design for Learning
      term_ja: 学習のユニバーサルデザイン
      abbreviation: UDL
      category: accessibility
      definition: |
        An approach to assessment design that provides multiple means of 
        engagement, representation, and expression from the outset.
      principles:
        - Multiple means of engagement
        - Multiple means of representation
        - Multiple means of action/expression
      in_assessment:
        - Varied question formats
        - Flexible response options
        - Built-in accessibility features
        - Clear, simple language
      related_terms:
        - Accommodation
        - Accessibility

    - term: WCAG
      term_ja: ウェブコンテンツアクセシビリティガイドライン
      category: accessibility
      definition: |
        Web Content Accessibility Guidelines - international standards 
        for making web content accessible to people with disabilities.
      principles:
        - Perceivable
        - Operable
        - Understandable
        - Robust
      conformance_levels:
        - A: Minimum
        - AA: Acceptable (commonly required)
        - AAA: Optimal
      relevant_criteria_for_assessment:
        - Keyboard navigation
        - Screen reader compatibility
        - Color contrast
        - Time limits with extensions
        - Error identification
      related_terms:
        - Accessibility
        - Universal Design

  # ==========================================
  # Cross-Reference Index
  # ==========================================
  cross_references:
    difficulty_measurement:
      - Difficulty Index
      - IRT (b parameter)
      - CTT
      
    discrimination_measurement:
      - Discrimination Index
      - Point-Biserial Correlation
      - IRT (a parameter)
      
    reliability_indicators:
      - Cronbach's Alpha
      - KR-20
      - Standard Error of Measurement
      - Test Information Function
      
    scoring_concepts:
      - Raw Score
      - Scaled Score
      - Cut Score
      - Percentile Rank
      
    item_quality:
      - Item Analysis
      - DIF
      - Distractor Analysis
      - Item Information Function
      
    test_security:
      - Proctoring
      - Browser Lockdown
      - Identity Verification
      - Question Pool
      
    accessibility_framework:
      - Accommodation
      - UDL
      - WCAG

  # ==========================================
  # Common Abbreviations Quick Reference
  # ==========================================
  abbreviations:
    - abbr: CTT
      full: Classical Test Theory
    - abbr: IRT
      full: Item Response Theory
    - abbr: DIF
      full: Differential Item Functioning
    - abbr: QTI
      full: Question and Test Interoperability
    - abbr: CAT
      full: Computer Adaptive Testing
    - abbr: SEM
      full: Standard Error of Measurement
    - abbr: 1PL
      full: One-Parameter Logistic (Rasch) Model
    - abbr: 2PL
      full: Two-Parameter Logistic Model
    - abbr: 3PL
      full: Three-Parameter Logistic Model
    - abbr: GRM
      full: Graded Response Model
    - abbr: MLE
      full: Maximum Likelihood Estimation
    - abbr: EAP
      full: Expected A Posteriori
    - abbr: MAP
      full: Maximum A Posteriori
    - abbr: KR-20
      full: Kuder-Richardson Formula 20
    - abbr: WCAG
      full: Web Content Accessibility Guidelines
    - abbr: UDL
      full: Universal Design for Learning
    - abbr: ADA
      full: Americans with Disabilities Act

